from colorama import Fore

from CANARY_SEFI.batch_manager import batch_manager
from CANARY_SEFI.core.function.check_adv_example import adv_example_da_check
from CANARY_SEFI.core.function.enum.step_enum import Step
from CANARY_SEFI.core.function.enum.transfer_attack_type_enum import TransferAttackType
from CANARY_SEFI.core.function.helper.realtime_reporter import reporter
from CANARY_SEFI.core.function.inference import inference, adv_inference
from CANARY_SEFI.core.function.generate_adv_example import build_AEs, explore_perturbation
from CANARY_SEFI.core.function.helper.batch_list_iterator import BatchListIterator
from CANARY_SEFI.evaluator.analyzer.synthetical_analyzer import \
    model_security_synthetical_capability_analyzer_and_evaluation
from CANARY_SEFI.evaluator.analyzer.test_data_analyzer import model_inference_capability_analyzer_and_evaluation, \
    attack_deflection_capability_analyzer_and_evaluation, attack_adv_example_da_analyzer_and_evaluation, \
    perturbation_explore_analyzer_and_evaluation
from CANARY_SEFI.evaluator.logger.attack_info_handler import find_attack_log_by_name_and_base_model, \
    find_attack_log_by_name


def adv_example_generate(dataset_info, attacker_list, attacker_config, model_config, img_proc_config):
    # 标记当前步骤
    batch_manager.sys_log_logger.set_step(Step.ADV_EXAMPLE_GENERATE)

    def function(atk_name, atk_args, model_name, model_args, img_proc_args):
        msg = "Generating Adv Example By Attack Method {} on(base) Model {}.".format(atk_name, model_name)
        reporter.console_log(msg, Fore.GREEN, show_batch=True, show_step_sequence=True)
        build_AEs(dataset_info, atk_name, atk_args, model_name, model_args, img_proc_args)

    BatchListIterator.attack_list_iterator(attacker_list, attacker_config, model_config, img_proc_config, function)


# 攻击对抗样本质量测试
def attack_adv_example_da_test(attacker_list, dataset_info, use_raw_nparray_data=False):
    # 标记当前步骤
    batch_manager.sys_log_logger.set_step(Step.ATTACK_ADV_EXAMPLE_DA_TEST)
    for atk_name in attacker_list:
        attack_logs = find_attack_log_by_name(atk_name)
        for attack_log in attack_logs:
            adv_example_da_check(attack_log,dataset_info,use_raw_nparray_data)


# 攻击对抗样本质量评估
def attack_adv_example_da_evaluation(attacker_list, use_raw_nparray_data=False):
    # 标记当前步骤
    batch_manager.sys_log_logger.set_step(Step.ATTACK_ADV_EXAMPLE_DA_EVALUATION)
    for atk_name in attacker_list:
        for base_model in attacker_list[atk_name]:
            attack_adv_example_da_analyzer_and_evaluation(atk_name, base_model, use_raw_nparray_data)


# 攻击偏转能力测试
def attack_deflection_capability_test(attacker_list, model_config, img_proc_config,
                                      transfer_attack_test=TransferAttackType.NOT,
                                      transfer_attack_test_on_model_list=None, use_raw_nparray_data=False):
    # 标记当前步骤
    batch_manager.sys_log_logger.set_step(Step.ATTACK_DEFLECTION_CAPABILITY_TEST)

    for atk_name in attacker_list:
        for base_model in attacker_list[atk_name]:

            test_on_model_list = []
            if transfer_attack_test == TransferAttackType.NOT:
                test_on_model_list.append(base_model)
            elif transfer_attack_test == TransferAttackType.APPOINT:
                test_on_model_list = transfer_attack_test_on_model_list[atk_name][base_model]
            elif transfer_attack_test == TransferAttackType.SELF_CROSS:
                test_on_model_list = attacker_list[atk_name]

            def function(model_name, model_args, img_proc_args):
                # 攻击测试
                msg = "Inferencing Adv Example(Generated by Attack Method({} *Base Model {}*)" \
                      "(Is use raw Numpy Array Data:{}))'s Label by Model {}" \
                    .format(atk_name, base_model, use_raw_nparray_data, model_name)
                reporter.console_log(msg, Fore.GREEN, show_batch=True, show_step_sequence=True)

                attack_log = find_attack_log_by_name_and_base_model(atk_name, base_model)
                adv_inference(attack_log, model_name, model_args, img_proc_args, use_raw_nparray_data)

            BatchListIterator.model_list_iterator(test_on_model_list, model_config, img_proc_config, function)


# 攻击偏转能力评估
def attack_deflection_capability_evaluation(attacker_list, use_raw_nparray_data=False):
    # 标记当前步骤
    batch_manager.sys_log_logger.set_step(Step.ATTACK_DEFLECTION_CAPABILITY_EVALUATION)
    for atk_name in attacker_list:
        for base_model in attacker_list[atk_name]:
            attack_deflection_capability_analyzer_and_evaluation(atk_name, base_model, use_raw_nparray_data)


# 模型推理能力测试
def model_inference_capability_test(dataset_info, model_list, model_config, img_proc_config):
    # 标记当前步骤
    batch_manager.sys_log_logger.set_step(Step.MODEL_INFERENCE_CAPABILITY_TEST)

    def function(model_name, model_args, img_proc_args):
        # 模型基线测试
        msg = "Inferencing Img Label by Model {}".format(model_name)
        reporter.console_log(msg, Fore.GREEN, show_batch=True, show_step_sequence=True)
        inference(dataset_info, model_name, model_args, img_proc_args)

    BatchListIterator.model_list_iterator(model_list, model_config, img_proc_config, function)


# 模型推理能力评估
def model_inference_capability_evaluation(model_list):
    # 标记当前步骤
    batch_manager.sys_log_logger.set_step(Step.MODEL_INFERENCE_CAPABILITY_EVALUATION)
    for model_name in model_list:
        model_inference_capability_analyzer_and_evaluation(model_name)


def model_security_synthetical_capability_evaluation(model_list, use_raw_nparray_data=False):
    # 标记当前步骤
    batch_manager.sys_log_logger.set_step(Step.MODEL_SECURITY_SYNTHETICAL_CAPABILITY_EVALUATION)
    for model_name in model_list:
        model_security_synthetical_capability_analyzer_and_evaluation(model_name, use_raw_nparray_data)


def explore_attack_perturbation(dataset_info, attacker_list, attacker_config, model_config, img_proc_config,
                                explore_perturbation_config):
    # 标记当前步骤
    batch_manager.sys_log_logger.set_step(Step.EXPLORE_ATTACK_PERTURBATION)

    def function(atk_name, atk_args, model_name, model_args, img_proc_args):
        msg = "攻击方法 {} 在模型 {} 上 依据扰动上下限分步生成上述样本的对抗样本并运行扰动测评"\
            .format(atk_name, model_name)
        reporter.console_log(msg, Fore.GREEN, show_batch=True, show_step_sequence=True)

        explore_perturbation_args = explore_perturbation_config.get(atk_name, None)
        explore_perturbation(dataset_info, atk_name, atk_args, model_name, model_args, img_proc_args,
                             explore_perturbation_args)

    BatchListIterator.attack_list_iterator(attacker_list, attacker_config, model_config, img_proc_config, function)


def explore_perturbation_attack_deflection_capability_test(attacker_list, model_config, img_proc_config, use_raw_nparray_data=False):
    # 标记当前步骤
    batch_manager.sys_log_logger.set_step(Step.EXPLORE_ATTACK_PERTURBATION_ATTACK_DEFLECTION_TEST)

    for atk_name in attacker_list:
        for base_model in attacker_list[atk_name]:
            def function(model_name, model_args, img_proc_args):
                # 攻击测试
                msg = "攻击方法 {} (基于 {} 模型) 在不同扰动限制下 生成的对抗样本 针对 {} 模型进行攻击测试".format(atk_name, base_model, model_name)
                reporter.console_log(msg, Fore.GREEN, show_batch=True, show_step_sequence=True)

                attack_logs = find_attack_log_by_name_and_base_model(atk_name, base_model, explore_perturbation_mode = True)

                for attack_log in attack_logs:
                    adv_inference(attack_log, model_name, model_args, img_proc_args, use_raw_nparray_data)

            BatchListIterator.model_list_iterator([base_model], model_config, img_proc_config, function)


# 攻击对抗样本质量测试
def explore_perturbation_attack_adv_example_da_test(attacker_list, dataset_info, use_raw_nparray_data=False):
    # 标记当前步骤
    batch_manager.sys_log_logger.set_step(Step.EXPLORE_ATTACK_PERTURBATION_ATTACK_ADV_EXAMPLE_DA_TEST)
    for atk_name in attacker_list:
        attack_logs = find_attack_log_by_name(atk_name)
        for attack_log in attack_logs:
            adv_example_da_check(attack_log, dataset_info, use_raw_nparray_data)


def explore_perturbation_attack_capability_evaluation(attacker_list, use_raw_nparray_data=False):
    # 标记当前步骤
    batch_manager.sys_log_logger.set_step(Step.EXPLORE_ATTACK_PERTURBATION_ATTACK_EVALUATION)
    for atk_name in attacker_list:
        for base_model in attacker_list[atk_name]:
            perturbation_explore_analyzer_and_evaluation(atk_name, base_model, use_raw_nparray_data)